## Week 8

### Introduction

There are redundancies in image. Some information in video cannot be observed
by humans so it can be dropped (in lossy).

This week we cover lossless compression.

Heart of encoding problem is representation of symbols generated by a source
into binary control words.

Cannot compress a signal below its entropy (Shannon theorem).

### Elements from information theory

Fixed length or variable length codes.

Not easy to measure the entropy or uncertainty of a source.

### Huffman coding

Symbols that occur more often have shorter codewords.

The two symbols that occur the least frequently will have the same length.

Huffman code is uniquely decodable.

### Run-length coding and fax

### Arithmetic coding

More efficient to generate codewords for groups (sequences of symbols). Block
coding.

With Huffman, in finding a codeword for a particular sequence of length m, we
need codewords for all possible sequences of length m.

With arithmetic coding, we can assign codewords to particular sequences without
having to generate codes for all sequences of that length.

### Dictionary techniques

In many applications, the output of the source consists of recurring patterns.

Keep a list (or dictionary) of frequently occurring patterns.

### Predictive coding

"past" signal values -> prediction model -> predicted signal values

